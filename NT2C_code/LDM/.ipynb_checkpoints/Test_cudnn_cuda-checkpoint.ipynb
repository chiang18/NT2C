{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U3xisc0bT-Et",
    "outputId": "d96578a2-bcef-4933-c0ff-4dbfaec6e357"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is cuda available? True\n",
      "11.8\n",
      "Is cuDNN version: 8907\n",
      "cuDNN enabled?  True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"Is cuda available?\", torch.cuda.is_available())\n",
    "print(torch.version.cuda)\n",
    "print(\"Is cuDNN version:\", torch.backends.cudnn.version())\n",
    "print(\"cuDNN enabled? \", torch.backends.cudnn.enabled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a-b-5aK7T-1o",
    "outputId": "462051ea-8ad4-41b3-ec01-56ff380ef7af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA GeForce RTX 2080 Ti\n",
      "GPU 1: NVIDIA GeForce RTX 2080 Ti\n",
      "GPU 2: NVIDIA GeForce RTX 2080 Ti\n",
      "GPU 3: NVIDIA GeForce RTX 2080 Ti\n",
      "Result of the tensor operation on GPU: tensor([2., 4.], device='cuda:0')\n",
      "Device of z: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# List all available GPUs\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "\n",
    "# Perform a simple operation on the GPU\n",
    "if torch.cuda.is_available():\n",
    "    # Create a tensor and move it to GPU\n",
    "    x = torch.tensor([1.0, 2.0]).cuda()\n",
    "    y = torch.tensor([1.0, 2.0]).cuda()\n",
    "\n",
    "    # Perform a simple operation\n",
    "    z = x + y\n",
    "    print(\"Result of the tensor operation on GPU:\", z)\n",
    "    print(\"Device of z:\", z.device)\n",
    "else:\n",
    "    print(\"CUDA is not available. Operations will run on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hpY90SQ6Umr0",
    "outputId": "ecaf6f03-3898-4e99-bf87-23e317f5341d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 13:33:39.148593: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-18 13:33:42.193134: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA available? True\n",
      "Available GPUs: 4 GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# Check if CUDA (GPU support) is available\n",
    "print(\"Is CUDA available?\", tf.test.is_built_with_cuda())\n",
    "\n",
    "# Check for GPU devices\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(\"Available GPUs:\", len(gpus), \"GPUs:\", gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YF62IrgGU31A",
    "outputId": "7e454c1b-0ea9-4bc0-dec0-e20582cef2fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: /physical_device:GPU:0\n",
      "GPU 1: /physical_device:GPU:1\n",
      "GPU 2: /physical_device:GPU:2\n",
      "GPU 3: /physical_device:GPU:3\n",
      "Result of the tensor operation on GPU: [2. 4.]\n",
      "Device of z: /job:localhost/replica:0/task:0/device:GPU:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 13:34:03.032493: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1985 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:1b:00.0, compute capability: 7.5\n",
      "2024-03-18 13:34:03.034209: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 2037 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:1e:00.0, compute capability: 7.5\n",
      "2024-03-18 13:34:03.036099: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 2037 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:89:00.0, compute capability: 7.5\n",
      "2024-03-18 13:34:03.037469: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 2037 MB memory:  -> device: 3, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:8c:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# List all available GPUs\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    for i, gpu in enumerate(gpus):\n",
    "        print(f\"GPU {i}: {gpu.name}\")\n",
    "else:\n",
    "    print(\"No GPUs found, operations will run on CPU.\")\n",
    "\n",
    "# Perform a simple operation on the GPU\n",
    "if gpus:\n",
    "    # TensorFlow automatically uses a GPU if available for operations,\n",
    "    # but you can explicitly set a device to ensure operations run on it.\n",
    "    with tf.device('/GPU:0'):  # This specifies the first GPU as the target device\n",
    "        # Create tensors\n",
    "        x = tf.constant([1.0, 2.0])\n",
    "        y = tf.constant([1.0, 2.0])\n",
    "\n",
    "        # Perform a simple operation\n",
    "        z = x + y\n",
    "\n",
    "    print(\"Result of the tensor operation on GPU:\", z.numpy())\n",
    "    # TensorFlow's Tensor objects have a .device attribute that shows the device placement\n",
    "    print(\"Device of z:\", z.device)\n",
    "else:\n",
    "    print(\"CUDA is not available. Operations will run on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
